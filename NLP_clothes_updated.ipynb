{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems Based On Amazon Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teammates: Nivetitha Ramachandar, Zhirou Zhang, Supriya Tiwari, Jiamin Zhu, Shruti Deshpande"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the outline of what we're going to do in this notebook.\n",
    "- Problem Statement,Objective\n",
    "- Overview and Benefits of review based engine\n",
    "- Dataset Overview\n",
    "- Data cleaning steps \n",
    "- Sentiment Analysis \n",
    "- Word2vec model \n",
    "- Recommendation Results\n",
    "- Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the bacis information of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reviewerID: ID of the reviewer\n",
    "- asin: ID of the product\n",
    "- reviewerName: name of the reviewer\n",
    "- helpful: helpfulness rating of the review, e.g. 2/3\n",
    "- reviewText:  text of the review\n",
    "- overall: rating score from 0 to 5\n",
    "- summary: summary of the review\n",
    "- unixReviewTime: time of the review (unix time)\n",
    "- reviewTime: time of the review (raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"reviews_Clothing_Shoes_and_Jewelry_5.json\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis, we'll extract only reviews text from the data and store it in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "USE_PREMADE_REVIEWS_TEXT = True\n",
    "\n",
    "from os import path\n",
    "reviews_text_filepath = 'medium/reviews_text.txt'\n",
    "if not USE_PREMADE_REVIEWS_TEXT:\n",
    "    with open(reviews_text_filepath, 'w') as f:\n",
    "        for review in data.reviewText.values:\n",
    "            # if the row lacks a review, skip it.\n",
    "            if pd.isnull(review):\n",
    "                continue\n",
    "            f.write(review + '\\n')\n",
    "else:\n",
    "    assert path.exists(reviews_text_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a simple function which helps us read each line from the reviews text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_reviews(filepath):\n",
    "    \"\"\"\n",
    "    helper function to read in the file and yield each line at a time.\n",
    "    \"\"\"\n",
    "    with open(filepath) as f:\n",
    "        for review in f:\n",
    "            yield review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "def retrieve_review(sample_num):\n",
    "    \"\"\"\n",
    "    get a specific review from reviews text file and return it.\n",
    "    \"\"\"\n",
    "    return next(islice(read_reviews(reviews_text_filepath), sample_num, sample_num+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a sample and see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_review = retrieve_review(200)\n",
    "sample_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT PROCESSING WITH SPACY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use spaCy to normalize reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import spacy\n",
    "# load english vocabulary and language models. This takes some time.\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(line):\n",
    "    \"\"\"\n",
    "    remove punctuation and whitespace.\n",
    "    \"\"\"\n",
    "    return [token.lemma_ for token in line \n",
    "                      if not token.is_punct and not token.is_space]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well spaCy did. Here's a normalized version of the sample review above. You can see that many words have been lowered & stemmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_review_normalized = lemmatize(nlp(sample_review))\n",
    "' '.join(sample_review_normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform normalizatioin for all the reviews we have. This takes a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "USE_PREMADE_SENTENCES_NORMALIZED = True\n",
    "\n",
    "sentences_normalized_filepath = 'medium/sentences_normalized.txt'\n",
    "\n",
    "\n",
    "if not USE_PREMADE_SENTENCES_NORMALIZED:\n",
    "    with open(sentences_normalized_filepath, 'w') as f:\n",
    "        for review_parsed in nlp.pipe(read_reviews(reviews_text_filepath)):\n",
    "            for sentence_parsed in review_parsed.sents:\n",
    "                lemmas = lemmatize(sentence_parsed)\n",
    "                f.write(' '.join(lemmas) + '\\n')\n",
    "else:\n",
    "    assert path.exists(sentences_normalized_filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are words which are often used together, and which get a special meaning when they're used together. We call them 'phrases'. We're now going to find bigram/trigram phrases from the reviews.\n",
    "\n",
    "To do so, we turn to the famous NLP library in Python, gensim. Particularly, the Phrases class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the normalized texts from the previous section, and build a bigram model upon them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "USE_PREMADE_BIGRAM_MODEL = True\n",
    "\n",
    "bigram_model_filepath = 'medium/bigram_model.dms'\n",
    "\n",
    "# gensim's LineSentence provies a convenient way to iterate over lines in a text file.\n",
    "# it outputs one line at a time, so you can save memory space. it works well with other gensim components.\n",
    "from gensim.models.word2vec import LineSentence\n",
    "# we take normalized sentences as unigram sentences, which means we didn't apply any phrase modeling yet.\n",
    "unigram_sentences = LineSentence(sentences_normalized_filepath)\n",
    "\n",
    "if not USE_PREMADE_BIGRAM_MODEL:    \n",
    "    \n",
    "    bigram_model = Phrases(unigram_sentences)\n",
    "    bigram_model.save(bigram_model_filepath)\n",
    "    \n",
    "else:\n",
    "    bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_review_bigram = bigram_model[sample_review_normalized]\n",
    "' '.join(sample_review_bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We process all the normalized texts in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "USE_PREMADE_BIGRAM_SENTENCES = True\n",
    "\n",
    "bigram_sentences_filepath = 'medium/bigram_sentences.txt'\n",
    "\n",
    "if not USE_PREMADE_BIGRAM_SENTENCES:\n",
    "    \n",
    "    with open(bigram_sentences_filepath, 'w') as f:\n",
    "        for unigram_sentence in unigram_sentences:\n",
    "            bigram_sentence = bigram_model[unigram_sentence]\n",
    "            f.write(' '.join(bigram_sentence) + '\\n')\n",
    "else:\n",
    "    assert path.exists(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take one step further. We're going to build a trigram phrase model on bigram model. It means, we can combine together two bigram phrases, or one unigram and one bigram phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "USE_PREMADE_TRIGRAM_MODEL = True\n",
    "\n",
    "trigram_model_filepath = 'medium/trigram_model.dms'\n",
    "\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import Phrases\n",
    "\n",
    "if not USE_PREMADE_TRIGRAM_MODEL:\n",
    "    \n",
    "    bigram_sentences = LineSentence(bigram_sentences_filepath)\n",
    "    trigram_model = Phrases(bigram_sentences)\n",
    "    trigram_model.save(trigram_model_filepath)\n",
    "\n",
    "else:\n",
    "    trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.cli.download(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import spacy\n",
    "# load english vocabulary and language models. This takes some time.\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "USE_PREMADE_REVIEWS_FOR_LDA = True\n",
    "\n",
    "reviews_for_lda_filepath = 'medium/reviews_for_lda.txt'\n",
    "\n",
    "if not USE_PREMADE_REVIEWS_FOR_LDA:\n",
    "    \n",
    "    with open(reviews_for_lda_filepath, 'w') as f:\n",
    "        \n",
    "        for review_parsed in nlp.pipe(read_reviews(reviews_text_filepath)):\n",
    "            \n",
    "            unigram_review = lemmatize(review_parsed)\n",
    "            bigram_review = bigram_model[unigram_review]\n",
    "            trigram_review = trigram_model[bigram_review]\n",
    "            # remove stop words\n",
    "            trimmed_review = [lemma for lemma in trigram_review \n",
    "                              if lemma not in spacy.lang.en.STOP_WORDS and lemma != '-PRON-']\n",
    "            f.write(' '.join(trimmed_review) + '\\n')\n",
    "else:\n",
    "    assert path.exists(reviews_for_lda_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "USE_PREMADE_SENTENCES_FOR_WORD2VEC = True\n",
    "\n",
    "sentences_for_word2vec_filepath = 'medium/sentences_for_word2vec.txt'\n",
    "\n",
    "if not USE_PREMADE_SENTENCES_FOR_WORD2VEC:\n",
    "    \n",
    "    with open(sentences_for_word2vec_filepath, 'w') as f:\n",
    "        for bigram_sentence in bigram_sentences:\n",
    "            trigram_sentence = trigram_model[bigram_sentence]\n",
    "            # remove stop words\n",
    "            trimmed_sentence = [lemma for lemma in trigram_sentence \n",
    "                                if lemma not in spacy.lang.en.STOP_WORDS and lemma != '-PRON-']\n",
    "            f.write(' '.join(trimmed_sentence) + '\\n')\n",
    "else:\n",
    "    assert path.exists(sentences_for_word2vec_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling with LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling is to automatically find topics from a bunch of documents - reviews, in this case. We'll now perform LDA, the most basic topic modeling method, on our reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary, MmCorpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to compile our dictionary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "USE_PREMADE_DICTIONARY = True\n",
    "\n",
    "dictionary_filepath = 'medium/dictionary.dict'\n",
    "\n",
    "if not USE_PREMADE_DICTIONARY:\n",
    "    \n",
    "    reviews_for_lda = LineSentence(reviews_for_lda_filepath)\n",
    "    dictionary = Dictionary(reviews_for_lda)\n",
    "    dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    dictionary.save(dictionary_filepath)\n",
    "else:\n",
    "    dictionary = Dictionary.load(dictionary_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we build a corpus which we'll use when performing LDA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "USE_PREMADE_CORPUS = True\n",
    "\n",
    "corpus_filepath = 'medium/corpus.mm'\n",
    "\n",
    "if not USE_PREMADE_CORPUS:\n",
    "    \n",
    "    def make_bow_corpus(filepath):\n",
    "        \"\"\"\n",
    "        generator function to read in reviews from the file\n",
    "        and output a bag-of-words represention of the text\n",
    "        \"\"\"\n",
    "        for review in LineSentence(filepath):\n",
    "            yield dictionary.doc2bow(review)\n",
    "            \n",
    "    MmCorpus.serialize(corpus_filepath, make_bow_corpus(reviews_for_lda_filepath))\n",
    "    \n",
    "review_corpus = MmCorpus(corpus_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can turn to gensim's LdaMulticore class for parallelized LDA, which is claimed to be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaMulticore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "USE_PREMADE_LDA = True\n",
    "\n",
    "lda_filepath = 'medium/lda.dms'\n",
    "\n",
    "if not USE_PREMADE_LDA:\n",
    "    \n",
    "    # number of workers should be set to your number of physical cores minus one\n",
    "    lda = LdaMulticore(review_corpus,\n",
    "                           num_topics=20,\n",
    "                           id2word=dictionary,\n",
    "                           workers=2)\n",
    "    lda.save(lda_filepath)\n",
    "else:\n",
    "    lda = LdaMulticore.load(lda_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect a specific topic from the model by it's index. There's no names for topics, though, because LDA is an unsupervised learning algorithm. Instead, you can see the words associated to the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.show_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#df_lda.distplot(df['petal_length'],kde = False)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on previous findings, we conduct sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy stuff\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en\n",
    "\n",
    "import spacy \n",
    "\n",
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = data\n",
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The customer overall rating is from 0 to 5. \n",
    "In order to simplify the problem we will split those into two categories: \n",
    "bad reviews: overall ratings < 3 \n",
    "good reviews: overall ratings >= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the label\n",
    "reviews_df[\"is_bad_review\"] = reviews_df[\"overall\"].apply(lambda x: 1 if x < 3 else 0)\n",
    "# select only relevant columns\n",
    "reviews_df = reviews_df[[\"reviewText\", \"is_bad_review\"]]\n",
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviews data is sampled in order to speed up computations.\n",
    "reviews_df = reviews_df.sample(frac = 0.1, replace = False, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the customer doesn't leave any negative/positive review, this will apprear as \"No negative\" or \"No positive\" in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 'No Negative' or 'No Positive' from text\n",
    "reviews_df[\"reviewText\"] = reviews_df[\"reviewText\"].apply(lambda x: x.replace(\"No Negative\", \"\").replace(\"No Positive\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the wordnet object value corresponding to the POS tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def clean_text(text):\n",
    "    # lower text\n",
    "    text = text.lower()\n",
    "    # tokenize text and remove puncutation\n",
    "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "    # remove words that contain numbers\n",
    "    text = [word for word in text if not any(c.isdigit() for c in word)]\n",
    "    # remove stop words\n",
    "    stop = stopwords.words('english')\n",
    "    text = [x for x in text if x not in stop]\n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "    # pos tag text\n",
    "    pos_tags = pos_tag(text)\n",
    "    # lemmatize text\n",
    "    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n",
    "    # remove words with only one letter\n",
    "    text = [t for t in text if len(t) > 1]\n",
    "    # join all\n",
    "    text = \" \".join(text)\n",
    "    return(text)\n",
    "\n",
    "# clean text data\n",
    "reviews_df[\"review_clean\"] = reviews_df[\"reviewText\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the process of cleanning reviewText data, we did following things:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lower the text\n",
    "- tokenize the text (split the text into words) and remove the punctuation\n",
    "- remove useless words that contain numbers\n",
    "- remove useless stop words like 'the', 'a' ,'this' etc.\n",
    "- Part-Of-Speech (POS) tagging: assign a tag to every word to define if it corresponds to a noun, a verb etc. using - the WordNet lexical database\n",
    "- lemmatize the text: transform every word into their root form (e.g. rooms -> room, slept -> sleep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Vader to conduct sentiment analysis. Vader uses a lexicon of words to find which ones are positives or negatives. It also takes into accout the context of the sentences to determine the sentiment scores.For each text, Vader retuns 4 values:\n",
    "\n",
    "- a neutrality score\n",
    "- a positivity score\n",
    "- a negativity score\n",
    "- an overall score that summarizes the previous scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will integrate those 4 values as features in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add sentiment anaylsis columns\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "reviews_df[\"sentiments\"] = reviews_df[\"reviewText\"].apply(lambda x: sid.polarity_scores(x))\n",
    "reviews_df = pd.concat([reviews_df.drop(['sentiments'], axis=1), reviews_df['sentiments'].apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we add some simple metrics for every text:\n",
    "- number of characters in the text\n",
    "- number of words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add number of characters column\n",
    "reviews_df[\"nb_chars\"] = reviews_df[\"reviewText\"].apply(lambda x: len(x))\n",
    "\n",
    "# add number of words column\n",
    "reviews_df[\"nb_words\"] = reviews_df[\"reviewText\"].apply(lambda x: len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step consist in extracting vector representations for every review. The module Gensim creates a numerical vector representation of every word in the corpus by using the contexts in which they appear (Word2Vec)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each text can also be transformed into numerical vectors using the word vectors (Doc2Vec). Same texts will also have similar representations and that is why we can use those vectors as training features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first have to train a Doc2Vec model by feeding in our text data. By applying this model on our reviews, we can get those representation vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create doc2vec vector columns\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(reviews_df[\"review_clean\"].apply(lambda x: x.split(\" \")))]\n",
    "\n",
    "# train a Doc2Vec model with our text data\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "\n",
    "# transform each document into a vector data\n",
    "doc2vec_df = reviews_df[\"review_clean\"].apply(lambda x: model.infer_vector(x.split(\" \"))).apply(pd.Series)\n",
    "doc2vec_df.columns = [\"doc2vec_vector_\" + str(x) for x in doc2vec_df.columns]\n",
    "reviews_df = pd.concat([reviews_df, doc2vec_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we add the TF-IDF (Term Frequency - Inverse Document Frequency) values for every word and every document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add TF-IDF columns for every word that appear in at least 10 different texts to filter some of them and reduce the size of the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add tf-idfs columns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(min_df = 10)\n",
    "tfidf_result = tfidf.fit_transform(reviews_df[\"review_clean\"]).toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_result, columns = tfidf.get_feature_names())\n",
    "tfidf_df.columns = [\"word_\" + str(x) for x in tfidf_df.columns]\n",
    "tfidf_df.index = reviews_df.index\n",
    "reviews_df = pd.concat([reviews_df, tfidf_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviewnew_df = reviews_df[reviews_df.columns[0:6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewnew_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of comparing raw review and clean review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewnew_df['reviewText'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewnew_df['review_clean'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have a better understanding of our data, we are going to exlpore more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show is_bad_review distribution\n",
    "reviews_df[\"is_bad_review\"].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this result, we can imply that our dataset is highly imbalanced because less than 10% of our reviews are consideres as negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# highest positive sentiment reviews (with more than 5 words)\n",
    "reviews_df[reviews_df[\"nb_words\"] >= 5].sort_values(\"pos\", ascending = False)[[\"reviewText\", \"pos\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the result, we found that most positive reviews are related to good feedbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowest negative sentiment reviews (with more than 5 words)\n",
    "reviews_df[reviews_df[\"nb_words\"] >= 5].sort_values(\"neg\", ascending = False)[[\"reviewText\", \"neg\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the result, all negative reviews correspond to bad feedbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sentiment distribution for positive and negative reviews\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "for x in [0, 1]:\n",
    "    subset = reviews_df[reviews_df['is_bad_review'] == x]\n",
    "    \n",
    "    # Draw the density plot\n",
    "    if x == 0:\n",
    "        label = \"Good reviews\"\n",
    "    else:\n",
    "        label = \"Bad reviews\"\n",
    "    sns.distplot(subset['compound'], hist = False, label = label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows the distribution of the reviews sentiments among goods reviews and bad ones. We can see that good reviews are for most of them considered as very positive by Vader. On the contrary, bad reviews tend to have lower compound sentiment scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vector modeling with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Word vector modeling (word embedding, put another way) is a method to transform words to vectors, which enables arithmetic with them. Word2Vec has been proposed by Google in 2013, and you can find python implementation of the model in ... gensim (of course!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "USE_PREMADE_WORD2VEC = True\n",
    "\n",
    "word2vec_filepath = 'medium/word2vec_model.dms'\n",
    "\n",
    "if not USE_PREMADE_WORD2VEC:\n",
    "    \n",
    "    sentences_for_word2vec = LineSentence(sentences_for_word2vec_filepath)\n",
    "    \n",
    "    # initiate the model with 100 dimensions of vectors, 5 words to look before and after each focus word, etc.\n",
    "    # and perform the first epoch of training\n",
    "    model = Word2Vec(sentences_for_word2vec, size=100, window=5, min_count=5, sg=1)\n",
    "    \n",
    "    # perform another 10 epochs of training\n",
    "    for _ in range(9):\n",
    "        model.train(sentences_for_word2vec,epochs=model.iter, total_examples=model.corpus_count)\n",
    "\n",
    "    model.save(word2vec_filepath)\n",
    "else:\n",
    "    model = Word2Vec.load(word2vec_filepath)\n",
    "model.init_sims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} training epochs so far.'.format(model.train_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transformed each word in our reviews to 100 dimentional vectors. Wonder how they look? here's word vectors in pandas dataframe form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take word vectors of most frequent words.\n",
    "num_words = 2000\n",
    "word_embeddings = pd.DataFrame(model.wv.syn0norm[:num_words, :], index=model.wv.index2word[:num_words])\n",
    "word_embeddings.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['adidas'], topn=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['shoe'], negative=['ugly'], topn=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Request key word insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_item=input(\"Please insert the key word of the product\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply most_similar method to output the most similar words corresponding to the key word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_word=model.most_similar(positive=[search_item],topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in search_word:\n",
    "    target_list.append(x[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the list contains top rates of related words through user given key word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuation and final target list of most related word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list2=[]\n",
    "#remove punctuation\n",
    "for x in target_list:\n",
    "    x=x.replace(\"_\",\" \")\n",
    "    target_list2.append(x)\n",
    "target_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the cleaned review contains the certain word within the target list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = reviewnew_df[reviewnew_df['review_clean'].str.contains(target_list2[3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.nlargest(10, 'pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all review has the word in target list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the dataframe contains all the related words that are in the target list\n",
    "for x in target_list2:\n",
    "    target_df = reviewnew_df[reviewnew_df['review_clean'].str.contains(x)]\n",
    "    target_df=target_df.append(target_df)\n",
    "target_df.sort_values(\"pos\", inplace = True) \n",
    "target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=target_df.nlargest(10, 'pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.drop_duplicates(subset='reviewText', keep='first')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the results leave the highest pos score from sentimental analysist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search back into original dataframe, maybe not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in final_df['reviewText']:\n",
    "    data_original=data_original.append(data.loc[data['reviewText'] == x])\n",
    "data_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of the suggested products ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original['asin']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
